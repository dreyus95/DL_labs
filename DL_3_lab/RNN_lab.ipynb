{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, batch_size=5, sequence_length=5):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_index = 0\n",
    "    \n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        chars, cnts = np.unique(list(data), return_index=True)\n",
    "        print(\"Chars: \",chars)\n",
    "        print(\"Counts: \",cnts)\n",
    "        self.sorted_chars = chars[np.argsort(-cnts)]\n",
    "        self.vocab_size = len(self.sorted_chars)\n",
    "        \n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_chars, range(len(self.sorted_chars)))) \n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "        print(\"Data: \", self.x)\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        return np.array([self.char2id[c] for c in sequence], dtype=np.int32)\n",
    "\n",
    "    def decode(self, encoded_sequence):\n",
    "        return [self.id2char[c] for c in encoded_sequence]\n",
    "    \n",
    "    def create_minibatches(self):\n",
    "        self.num_batches = int((len(self.x) - 1) / (self.batch_size * self.sequence_length)) # calculate the number of batches\n",
    "        self.num_batches = max(1, self.num_batches)\n",
    "        print (\"Number of batches: \", self.num_batches)\n",
    "        print (\"Batch size: \", self.batch_size)\n",
    "        print(\"Sequence length: \", self.sequence_length)\n",
    "\n",
    "        # Is all the data going to be present in the batches? Why?\n",
    "        # What happens if we select a batch size and sequence length larger than the length of the data?\n",
    "        \n",
    "        # No, maybe we select batch_size and sequence_length numbers that can't take all the data\n",
    "        # the program will break, maybe if we filled the input data with most frequent char or smth\n",
    "\n",
    "        #######################################\n",
    "        #       Convert data to batches       #\n",
    "        #######################################\n",
    "        batch_chars = self.batch_size * self.sequence_length\n",
    "        self.batches = np.zeros([self.num_batches, self.batch_size, self.sequence_length + 1], dtype=np.int32) \n",
    "        self.batch_index = 0\n",
    "        print(\"Data size:\" , len(self.x))\n",
    "        \n",
    "        for n in range(self.num_batches):\n",
    "            for s in range(self.batch_size):\n",
    "                sent_start = s * (self.sequence_length * self.num_batches)\n",
    "                start = n * self.sequence_length + sent_start\n",
    "                end = start + self.sequence_length + 1 \n",
    "                self.batches[n, s, :] = self.x[start:end]\n",
    "        \n",
    "    \n",
    "    def next_minibatch(self):\n",
    "        # ...\n",
    "        # handling batch pointer & reset\n",
    "        # new_epoch is a boolean indicating if the batch pointer was reset\n",
    "        # in this function call\n",
    "        batch_x, batch_y = None, None\n",
    "        new_epoch = self.batch_index == self.num_batches\n",
    "        if new_epoch:\n",
    "            self.batch_index = 0\n",
    "        batch = self.batches[self.batch_index, :, :]\n",
    "        self.batch_index += 1\n",
    "        batch_x = batch[:, :-1]\n",
    "        batch_y = batch[:, 1:]        \n",
    "        return new_epoch, batch_x, batch_y\n",
    "    \n",
    "    def _as_one_hot(self, x, vocabulary_size):\n",
    "        Yoh = np.zeros((len(x), vocabulary_size))\n",
    "        Yoh[np.arange(len(x)), x] = 1\n",
    "        return Yoh\n",
    "    \n",
    "    def one_hot(self, batch):\n",
    "        if batch.ndim == 1:\n",
    "            return self._as_one_hot(batch, self.vocab_size)\n",
    "        else:\n",
    "            return np.array([self._as_one_hot(s, self.vocab_size) for s in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Testing Dataset Class </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars:  ['\\n' ' ' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p'\n",
      " 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
      "Counts:  [21 27  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23\n",
      " 24 25 26]\n",
      "Data:  [27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n",
      "  2  1  0 27 26 25 24 23 22 21 20 19 18 17 16]\n",
      "Original:  hello there\n",
      "Encoded:  [20 23 16 16 13  0  8 20 23 10 23]\n",
      "decoded:  ['h', 'e', 'l', 'l', 'o', ' ', 't', 'h', 'e', 'r', 'e']\n",
      "\n",
      "\n",
      "\n",
      "Number of batches:  1\n",
      "Batch size:  4\n",
      "Sequence length:  5\n",
      "Data size: 40\n",
      "S: [[27 26 25 24 23]\n",
      " [22 21 20 19 18]\n",
      " [17 16 15 14 13]\n",
      " [12 11 10  9  8]]\n",
      "X [['a', 'b', 'c', 'd', 'e'], ['f', 'g', 'h', 'i', 'j'], ['k', 'l', 'm', 'n', 'o'], ['p', 'q', 'r', 's', 't']]\n",
      "T: [[26 25 24 23 22]\n",
      " [21 20 19 18 17]\n",
      " [16 15 14 13 12]\n",
      " [11 10  9  8  7]]\n",
      "Y [['b', 'c', 'd', 'e', 'f'], ['g', 'h', 'i', 'j', 'k'], ['l', 'm', 'n', 'o', 'p'], ['q', 'r', 's', 't', 'u']]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(batch_size=4, sequence_length=5)\n",
    "dataset.preprocess(\"test.txt\")\n",
    "txt = \"hello there\"\n",
    "encoded = dataset.encode(txt)\n",
    "decoded = dataset.decode(encoded)\n",
    "\n",
    "print(\"Original: \", txt)\n",
    "print(\"Encoded: \", encoded)\n",
    "print(\"decoded: \", decoded)\n",
    "print(\"\\n\\n\")\n",
    "dataset.create_minibatches()\n",
    "for i in range(dataset.num_batches):\n",
    "    _, s, t = dataset.next_minibatch()\n",
    "    print(\"S:\", s)\n",
    "    print(\"X\", list(map(dataset.decode, s)))\n",
    "    print(\"T:\",t)\n",
    "    print(\"Y\", list(map(dataset.decode, t)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, hidden_size=100, sequence_length=30, vocab_size=100, learning_rate=1e-1):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.U = np.random.normal(size=[vocab_size, hidden_size], scale=2.0 / np.sqrt(hidden_size)) # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=2.0 / np.sqrt(hidden_size)) # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size]) # ... input bias\n",
    "\n",
    "        self.V = np.random.normal(size=[hidden_size, vocab_size], scale=2.0 / np.sqrt(vocab_size)) # ... output projection\n",
    "        self.c = np.zeros([1, vocab_size]) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache =  (W, x, h_prev, h_current)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "\n",
    "        return h_current, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        hs, caches = [h0], []\n",
    "        for T in range(self.sequence_length):\n",
    "            minibatch = x[:, T, :]\n",
    "            h_curr, cache_curr = self.rnn_step_forward(minibatch, hs[-1], U, W, b)\n",
    "            hs.append(h_curr)\n",
    "            caches.append(cache_curr)\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        hs = np.array(hs[1:]).transpose((1, 0, 2)) # skip initial state\n",
    "        return hs, caches\n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "        W, x, h_prev, h_curr = cache\n",
    "        \n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        dLa = grad_next * (1 - h_curr**2)\n",
    "    \n",
    "        dh_prev = np.dot(dLa, W.T)\n",
    "        dU = np.dot(x.T, dLa)\n",
    "        dW = np.dot(h_prev.T, dLa)\n",
    "        db = np.sum(dLa, axis=0)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "\n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU, dW, db = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "        grad_next = np.zeros_like(dh[0])\n",
    "        for dh_T, cache_T in reversed(list(zip(dh, cache))):\n",
    "            grad_next, dU_T, dW_T, db_T = self.rnn_step_backward(dh_T + grad_next, cache_T)\n",
    "            dU += dU_T\n",
    "            dW += dW_T\n",
    "            db += db_T\n",
    "            \n",
    "        return np.clip(dU, -5, 5), np.clip(dW, -5, 5), np.clip(db, -5, 5)\n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        o = np.dot(h, V) + c\n",
    "        exp = np.exp(o)\n",
    "        softmax = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return softmax, o\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "        loss, dhs, dV, dc = 0.0, [], np.zeros_like(self.V), np.zeros_like(self.c)\n",
    "        \n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a tensor of dimension \n",
    "        #     batch_size x sequence_length x vocabulary size - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[batch_id][timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[batch_id][timestep][batch_y[timestep]] = 1\n",
    "        #     where y might be a list or a dictionary.\n",
    "    \n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        # calculate yhat - softmax of the output\n",
    "        # calculate the cross-entropy loss\n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        N = len(h)\n",
    "        for T in range(self.sequence_length):\n",
    "            yp = y[:, T, :]\n",
    "            h_T = h[:, T, :]\n",
    "            \n",
    "            softmax, o = self.output(h_T, V, c)\n",
    "            \n",
    "            loss += -np.sum(np.log(softmax)*yp) / N\n",
    "            dO = (softmax - yp) / N\n",
    "            \n",
    "            dV += np.dot(h_T.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dh_T = np.dot(dO, V.T)\n",
    "            dhs.append(dh_T)\n",
    "        \n",
    "        return loss, dhs, dV, dc\n",
    "    \n",
    "    \n",
    "    def update(self, dU, dW, db, dV, dc):\n",
    "\n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        parameters = [self.U, self.W, self.b, self.V, self.c]\n",
    "        derivatives = [dU, dW, db, dV, dc]\n",
    "        memories = [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]\n",
    "        \n",
    "        for x, dx, mem_x in zip(parameters, derivatives, memories):\n",
    "            mem_x += np.square(dx)\n",
    "            x -= (self.learning_rate * dx) / np.sqrt(mem_x + 1e-7) # adding 1e-7 to avoid division with 0\n",
    "    \n",
    "    \n",
    "            \n",
    "    def step(self, h, x, y):\n",
    "        h, cache = self.rnn_forward(x, h, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        self.update(dU, dW, db, dV, dc)\n",
    "        return loss, h[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Run Method </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = \"./rnn_model\"\n",
    "\n",
    "def run_language_model(dataset, max_epochs=1000, hidden_size=100, sequence_length=30, learning_rate=1e-1, print_every=5):\n",
    "    \n",
    "        vocab_size = len(dataset.sorted_chars)\n",
    "        rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate) # initialize the recurrent network\n",
    "\n",
    "        current_epoch = 0 \n",
    "        batch = 0\n",
    "\n",
    "        h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "\n",
    "        best_loss = 9999\n",
    "\n",
    "        while current_epoch < max_epochs: \n",
    "            e, x, y = dataset.next_minibatch()\n",
    "\n",
    "            if e: \n",
    "                current_epoch += 1\n",
    "                batch = 0\n",
    "                h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "                # why do we reset the hidden state here?\n",
    "\n",
    "            # One-hot transform the x and y batches\n",
    "            x_oh, y_oh = dataset.one_hot(x), dataset.one_hot(y)\n",
    "\n",
    "            # Run the recurrent network on the current batch\n",
    "            # Since we are using windows of a short length of characters,\n",
    "            # the step function should return the hidden state at the end\n",
    "            # of the unroll. You should then use that hidden state as the\n",
    "            # input for the next minibatch. In this way, we artificially\n",
    "            # preserve context between batches.\n",
    "            loss, h0 = rnn.step(h0, x_oh, y_oh)            \n",
    "            \n",
    "            if batch % print_every == 0: \n",
    "                seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "                n_sample = 300\n",
    "                sampled = sample(rnn, seed, n_sample, dataset)\n",
    "                print(''.join(sampled))\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    with open(save_path, \"wb\") as f:\n",
    "                        pickle.dump(rnn, f)\n",
    "                        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                        print('> Saving to:', save_path)\n",
    "                else:\n",
    "                    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                    print (\"Current loss is not better than previous one\")\n",
    "                    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                print(\"epoch: %06d:\\t\" % (current_epoch), end=\"\")\n",
    "                print(\"batch: %06d:\\t\" % (batch), end=\"\")\n",
    "                print(\"Current batch loss: %.4f\" % (loss))\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            \n",
    "            batch += 1\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Sampling method <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(rnn, seed, n_sample, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    # run a step for every char in the seed\n",
    "    for c_oh in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(c_oh.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(c_oh))\n",
    "        \n",
    "    for i in range(len(seed), n_sample):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        softmax, o = rnn.output(h0, rnn.V, rnn.c)\n",
    "        # pick a char based on their probability score\n",
    "        out_char_oh = np.random.choice(range(dataset.vocab_size), p=softmax.ravel()) \n",
    "        sampled.append(out_char_oh)\n",
    "  \n",
    "    return dataset.decode(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars:  ['\\n' ' ' '!' \"'\" ',' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' '?'\n",
      " 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n",
      " 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' '`' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i'\n",
      " 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
      "Counts:  [     8     14    150     18     13    110  33239  62403  63554 122081\n",
      "  33237  33238  62402  91181  93135  93136      7     73    114     31\n",
      "    103    595      5     26   1809      1      2      9    116      4\n",
      "     75    175   1692   6685  72257      3      0    154   1689   1792\n",
      "    825  97809      6   6010 278577     10     58     11     29     28\n",
      "     34     96     22     15    320     12     32    198     17     40\n",
      "    462    891     27     16     19     60     41    331   1330     77\n",
      "   1033]\n",
      "Data:  [70 69 68 ..., 33 62 62]\n",
      "Number of batches:  3947\n",
      "Batch size:  5\n",
      "Sequence length:  30\n",
      "Data size: 592181\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(batch_size=5, sequence_length=30)\n",
    "dataset.preprocess(\"dataset/selected_conversations.txt\")\n",
    "dataset.create_minibatches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LqTQaPIhoIDsc\n",
      "U hLS0LJtWa4q\n",
      "narLIl3'0cNtnhBcYB\n",
      "0Q'dl0BL\n",
      "?LEuj\n",
      "z?hwbAc'CMSTbh,b`RI\n",
      "uvGBNtkS:bx5L0,r\n",
      "wFrc'`gJEo0Y9\n",
      "Rn\n",
      "B'FfIlW\n",
      "W4\n",
      "VL\n",
      "4brmx\n",
      "VfupJTLNwV:dLMRMksoub2fyaUIuBS\n",
      "rIFLSPU\n",
      "4L`ucxH?\n",
      "c Yg'lI\n",
      "pcaCL76c\n",
      "RN\n",
      "LbMyNt4YGGamqtISl3:RIacqvu1\n",
      "WahQj`Q!TUSPH'uz\n",
      "4qT.SvUahq\n",
      "RYvW5AOqoU1Yc\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 000000:\tCurrent batch loss: 170.7319\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MPF33N3P8X83383762F85337KQ266Xk\n",
      ".\n",
      "\n",
      "J8n5s3188j4X339QXb,`Q,27788\n",
      "\n",
      "8X33XXzs17Q3z`8z?\n",
      "`J8KQ1166o8X3B`Pz3Q83ZmyoQinhonQyaj431378B,733828aa4sxDE:\n",
      "`88`\n",
      "3OV1`5`Qwht!s'314\n",
      "`Wh9`9K`\n",
      "Y78h58Qju3fiQlrlave5`z5`go6thethap2ttem.\n",
      "\n",
      "z28`X3`81`P6uf`8n?\n",
      "Lqpd48a`Xx9nlthe5i7y59lzy5h555emX84CH372\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 000975:\tCurrent batch loss: 62.3455\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "38768X48858`84XX3Y88``87I87\n",
      "8SX8`vaXxdj'th05`\n",
      "3`V375``juvove h`349587x896Q`o8\n",
      "\n",
      "H`85303M08X88N`REXF:\n",
      "WRI8XN3`682987657885thi5!\n",
      "\n",
      "j5m8mXqu68608`58803`XnX5XX2X`6QX88528`68663`4Xqu5`Xathaneeninknvive\n",
      "HHH`3KA3`\n",
      "8X63`55XX3`78`\n",
      "8`6hexaWhan5thistjusigh?\n",
      "SN`888PX398850316883Q1X3Q0QQ\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 001950:\tCurrent batch loss: 63.6946\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "C86quthin, Nha7?\n",
      "\n",
      "D6`48\n",
      "KaVKA`R8`5`6g9Q`Q679`4`\n",
      "\n",
      "`B866Q`XmapquJstith\n",
      "\n",
      "``8Xizandiny highthe tfoteessthessughaotnesst 2p8w5st6D`KX`8`QC1`Z7``U\n",
      "\n",
      "ZO3WAR3P4`98XPYOXKE:\n",
      "Kviz3`X9oxziatetunbnesqu8kju`J3srivr\n",
      "C88\n",
      "Jq9`683`884XP\n",
      "382\n",
      "6X54V,hay88\n",
      "\n",
      "`38X`88X6juvx5zouzist\n",
      "\n",
      "KU`Na\n",
      "XSj8Z76xJ\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 002925:\tCurrent batch loss: 64.2338\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "Xum2\n",
      "`18`bo6zughiZhpQ6rinad4xy,\n",
      "\n",
      "JxxX6TEQPIHE7QJZ8`6X`K``5WUKE``86`67:\n",
      "33``600K:\n",
      "Sx1`WhLd6KTXRO`742:\n",
      "WUV66NN`:\n",
      "Y367OSKU`K4`:\n",
      "J605K:\n",
      "Quthithabonguthdad4nywootinighithettime6\n",
      "vit?\n",
      "\n",
      "P63Z`Q`Wh66YQ3K:\n",
      "W`K3E:\n",
      "OBKUR:\n",
      "K666Q`2Q``3CK6WE``9Z66KO6KA`QPB49A`N0WO:\n",
      "CZ8X`O8K474`IT:\n",
      "Hq`2G`\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 003900:\tCurrent batch loss: 57.4401\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "hat?\n",
      "\n",
      "LUKA:\n",
      "Jxhevecrbastn hit dtthssihnitho ghy it. gfiggotnit ttchthedt gnetcnttgtchthetnisgihc thtcit whh thtsttstgddeghttt? fwsttght h? fc, bedefcfndthgththdshd?? thet rlddded!f,tnhtf thkndtnthdstththththshfth7? cddeddcthsht dhstttt thdhdhndthghdt ghdtthdstddnththwthhdt\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 000000:\tCurrent batch loss: 109.4742\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "L7AVIRE:\n",
      "I mans?\n",
      "\n",
      "MAVALATEY:\n",
      "J.\n",
      "\n",
      "JItE:\n",
      "Brt meed of nonkoo on't laveere?\n",
      "\n",
      "VARSCK:\n",
      "Buve in you? Gurk. There emt in to?\n",
      "\n",
      "AVIRLCHIKK:\n",
      "Nou rou'ld. Bre.\n",
      "VEt Sheanx fut bougipf the caed th'. Avo hilkela wartny.\n",
      "\n",
      "MAVERIA:\n",
      "Heryss theun sfoft it rad lingas jot yoo migwasted on a le \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 000975:\tCurrent batch loss: 54.7744\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SOLUSAN:\n",
      "Yoe stelt isb.... Yfuckers nis the farkre wharcreowdy is ins bat uug Terong. AttE tome.\n",
      "\n",
      "CWAD:\n",
      "Huckbfhe the isn bes t gow, doingay me ofe.\n",
      "\n",
      "BON:\n",
      "Is, than't a tnixiyw lave well tod meing op!\n",
      "\n",
      "DHK:\n",
      "You dids it mito thist youd I've Ge'se?\n",
      "\n",
      "TAPONG:\n",
      "\n",
      "TEUD:\n",
      "Yove che loo\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 001950:\tCurrent batch loss: 59.9482\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DACKOHSIAG TYER:\n",
      "Yise padge. Wu tcouln yom\n",
      "\n",
      "MBATSI MAVETTY:\n",
      "I dick of Bion hhe it co hat tell bofrey cco yous loy th'm dasvendassentind thuno, TI'r Souch, tom..\n",
      "\n",
      "UMCHED:\n",
      "But ang tablatredalve gight.\n",
      "\n",
      "CHELTY:\n",
      "How Beler thure rebofmee patull toboushnsm\n",
      "\n",
      "THETIGTSMUKEY:\n",
      "De.  y\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 002925:\tCurrent batch loss: 61.0147\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RMIND:\n",
      "Keabe wood. Youl that nas It't ak, beve bet thind youd nang ret the gatum and, uh I gowd yo hive. b5e fuoke I'dr\n",
      "\n",
      "HENDIDDY:\n",
      "Well and. Wh, Henther bpepbe hack.\n",
      "\n",
      "DATT:\n",
      "Whis I cile in't Crasime ckel, bing . Doee Gelres it on ar soon't whe Bowhing then hoves now tho did\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 003900:\tCurrent batch loss: 55.6237\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ANDIA:\n",
      "I'm couy fore the ghat kor andiry I jimpranvion,then mp getre fuld se the dring go don il is.\n",
      "\n",
      "DUKGAR:\n",
      "I'm radl tor! .\n",
      "\n",
      "RAMR:\n",
      "Not reatimg bugrach.\n",
      "\n",
      "LON:\n",
      "Nower  I fore then'm ghit, Perow sceroy here.\n",
      "\n",
      "DONDY:\n",
      "But.\n",
      "\n",
      "LAND:\n",
      "Vo Jreveyif ging anece your tt te ofie that's b\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 000000:\tCurrent batch loss: 77.1629\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "JY:\n",
      "Yeul ant me shond wates?\n",
      "\n",
      "JEFICK:\n",
      "I krogudn  foukre id enmepen atild a it massto doue!\n",
      "\n",
      "MR ITNI MeNI I Joywe till be youls. moke doter a kno way are who mutole fillt in woult bown it!  I? Je in hpas.\n",
      "\n",
      "DOBKIY:\n",
      "Whatabn. You you? Yerays ole un dech.\n",
      "\n",
      "VERIY:\n",
      "I Und ghe mibo\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 000975:\tCurrent batch loss: 52.4943\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DEYSA:\n",
      "Detaintites to ao we'lpusrabbue the, the 4cous?\n",
      "\n",
      "DECMANRICKE:\n",
      "Com le youn garem a ofry ald firnt.\n",
      "\n",
      "PRAND:\n",
      "Bere the dar ableamile lat, waide dger there fuckis a tho're to gunste in.  I veree gori?\n",
      "\n",
      "EGONN:\n",
      "Detant daidbamyo, Ewt the don jusk I't, doak antr?\n",
      "TEL2:\n",
      "Doise\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 001950:\tCurrent batch loss: 58.9966\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MrUKAR:\n",
      "Whe Tiden?\n",
      "\n",
      "PRUSTER:\n",
      "Yes ifdet a7s wa ut.\n",
      "\n",
      "BENRIE:\n",
      "Net got ho?\n",
      "\n",
      "KORNER:\n",
      "Ropeagety... to pell us!\n",
      "\n",
      "HEREM:\n",
      "You that ipes... or want or dares...\n",
      "\n",
      "INNAE:\n",
      "Yet. Sferored fon't  now where  Longt me asheas oo that all did.  I dot how on give,end som of ared or shast ast. P\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 002925:\tCurrent batch loss: 59.6406\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EDENTTIHA:\n",
      "Bucks duy anople I wish have it.\n",
      "\n",
      "DUKE:\n",
      "You don'in' copoore, the that.\n",
      "\n",
      "CRENDIE:\n",
      "Goon theat can'ts Me. Noveres.\n",
      "\n",
      "VI MY:\n",
      "Houy?\n",
      "\n",
      "ETTY:\n",
      "Oh nrem ingurestathen whit dentostay have cas.  this sueed wath you's thy for you for dow... gensitain'. I. Rtabreem on ahouy man\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 003900:\tCurrent batch loss: 52.2120\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HUKDY:\n",
      "Bof you fusn berill fried?\n",
      "\n",
      " ANAY:\n",
      "Hentace That the.\n",
      "\n",
      "DYRINICO:\n",
      "I'me cail that dajwing wiledituretwave and to colldsto. \n",
      "OSGOR:\n",
      "You're to the have tomang and somes dyill.\n",
      "\n",
      "PRINEY:\n",
      "Goch muy, Don wholpld ous syhermastlilg ackeackant, 6otfrUt Bep lid, having you surco \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 000000:\tCurrent batch loss: 75.0727\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELIGE:\n",
      "You?\n",
      "\n",
      "RAMINT:\n",
      "I's ut. Ena woll could ssoprease the're on me.\n",
      "\n",
      "JAELL:\n",
      "Where what thive Bifne shome ganene come. Rouchens a or to ching as be th wott ho wwhat manve any fill the beer couk?\n",
      "\n",
      "MANLY:\n",
      "Ar worct dors likdimstat, shtyod ur the met cane.\n",
      "\n",
      "PIAMA:\n",
      "I's wat nover\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 000975:\tCurrent batch loss: 50.8561\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LUKOR:\n",
      "Of, Keare. Bucked upponurend fat to  he dongitinge tawave haning, Whyron that gost nowung, werdy wank, huppure, Sheffreingyy doo, bxeeball he niged bel and trartast it all thas gow.\n",
      "\n",
      "CHARLEI:\n",
      "Were a rid I dou be hould even, norilns offrease a doup, a lot, sayy, Peft\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 001950:\tCurrent batch loss: 57.9385\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ESTOREIS:\n",
      "Eyboe loking dom.\n",
      "\n",
      "SARBA:\n",
      "Nos. Yoo pungod abletul hank of a crut's swith stankent faves meken your ste not tobchne, belfy I  harm?\n",
      "\n",
      "MONZO:\n",
      "Can pllimiveston't top to?\n",
      "\n",
      "LURAG:\n",
      "I jut take wanking you dedlen, I hevieen, Thery woy his of you soing lunen't...o want of \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 002925:\tCurrent batch loss: 58.1307\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GORAH:\n",
      "Uh wim dikan him me your have velp makn.\n",
      "\n",
      "LANEON:\n",
      "Han you're portt, lageden, Gale as who Bake gain tha He'f sirt town.\n",
      "\n",
      "BENDY:\n",
      "Rabpech. Anew?\n",
      "\n",
      "DON:\n",
      "Res.\n",
      "\n",
      "LEGET:\n",
      "You Bankiz.\n",
      "\n",
      "ENDE:\n",
      "Dottiriems.\n",
      "\n",
      "LEXOWER:\n",
      "Gook you's tha wirl.\n",
      "\n",
      "DR. PAN:\n",
      "I's  bray you sy'sc tich.\n",
      "\n",
      "DUN:\n",
      "W\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 003900:\tCurrent batch loss: 50.6354\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DOHTRBEISOO:\n",
      "Werne maverin.  he bred, wo daall ba `uh. Yeal the you rich, him thistr. Ecdece soilrtures Gands we're. I's the govee shoungulnigringef af feel dotenr.\n",
      "\n",
      "EDON:\n",
      "Now the hi. .\n",
      "\n",
      "LIA:\n",
      "You tuln you apoond.\n",
      "\n",
      "DR. THTE:\n",
      "kins!  Machsrevs got me's guysigile haves?\n",
      "\n",
      "REOMO\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 000000:\tCurrent batch loss: 74.6593\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LEASBE:\n",
      "I gome, it his I hon't shad it besting? Rour sover realln, I's you ran.\n",
      "\n",
      "TUKE TIBly'll your ghis the reversem pare bughthink stoiedined ainidn in ane theak, be you? recasin.\n",
      "\n",
      "SLIKA:\n",
      "But fire furt ul gais.\n",
      "\n",
      "TED:\n",
      "I car domicgreldave.a bight af ofugh these, Ma bet ent\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 000975:\tCurrent batch loss: 50.4034\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "INK:\n",
      "poly that immecade heriyw I should ore going?  We'te thery oves to butss the Mredvere the non't goither ablorn's as ane you just your, saran' me you're and barmid iving eroued, shark am mes. Vohink it tor fre umy ahan suckinn't thes and Nus but a dart.\n",
      "\n",
      "MAND:\n",
      "Love the\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 001950:\tCurrent batch loss: 57.1106\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CARTH:\n",
      "Batyron't oo got  it ohe.... Strouted ous to colluco to stald got what to you cad freading in that to we blazen this meas maltens untore got or ot we knotwand araie. I jugut btaket feaf! Yis Sor singh 298vererstern. Sight ton't fingut nen. I'llu'vinging she diut anf\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 002925:\tCurrent batch loss: 57.1885\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CORAY:\n",
      "Conaitrer. I sere.\n",
      "\n",
      "DONNIE:\n",
      "No.  Ol, bevery you la!\n",
      "\n",
      "HAND:\n",
      "Call hom me... rak, in the wiro, here'f be this.\n",
      "\n",
      "DONDENTO:\n",
      "Yeve mortedrech and to piabidgerm.\n",
      "\n",
      "LROSTOR:\n",
      "Yex I was?..\n",
      "\n",
      "ICHETHWO:\n",
      "HowR. Ant your willabe.in'tte to befingtay.\n",
      "\n",
      "RUKE:\n",
      "Fat it' ardand a now there \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 003900:\tCurrent batch loss: 49.1003\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RODDIE:\n",
      "Hus?\n",
      "\n",
      "HADD:\n",
      "Noos.\n",
      "\n",
      "LOUNTE:\n",
      "To stull hen abet lady achatan's un yif ok, What all sulk aday, make yourkenter to tume jutcwring a weed just I Nry kand htllit lackaing her sel he.\n",
      "\n",
      "TORHY:\n",
      "You be? Lour Fatt think than Jotr.\n",
      "\n",
      "IDDEY:\n",
      "Do...\n",
      "\n",
      "PRIGEE:\n",
      "I wiatl wast whtradnts \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 000000:\tCurrent batch loss: 72.0743\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MARLETR:\n",
      "You tlit?\n",
      "\n",
      "COBLY:\n",
      "Oh, with do beet that oney he pick wall up of wene a messs can,   What wan, Whing my reot this de's upymane My donennink I can ofr. Thet'd enecrybassed be it meastifa? I fon promes shen's gom, at in you pic it it. I'm sto lot chung.  I was the no\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 000975:\tCurrent batch loss: 49.5201\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "FREDY:\n",
      "That apesecced, be's retade. Hereane for... Othe bight it ourd lidad that, well won a mever marting on your lo Dongng got, tRear he wash id juKk. I'rbugh to Mandwen thanfeajlst?  no, Refarce?\n",
      "\n",
      "HAN:\n",
      "Nounfy. I've tobetrewerlyerve yourver, go stard won't we don'y be tt\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 001950:\tCurrent batch loss: 56.4205\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CORBERY:\n",
      "Thourser your fich of int the the prine.  Ire your?\n",
      "\n",
      "TRYHER:\n",
      "Who cas shist?\n",
      "\n",
      "JODN:\n",
      "Hoike, That's of beywirned.\n",
      "\n",
      "LEIAHOO:\n",
      "He'sbs it.  whenriss this one sount pian owings.\n",
      "\n",
      "CHECHERESTHRITE:\n",
      "If you'll ofite, I himer.\n",
      "\n",
      "STRYAOT:\n",
      "Ho mine tor I my Got'll bah lebping feri\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 002925:\tCurrent batch loss: 57.0643\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "INDER:\n",
      "Nobl tom that y. You fucc at enac. MOLERDY:\n",
      "Upe oon'y.  you must it's What thean out reuten jood.\n",
      "\n",
      "DENAO:\n",
      "This mill wead ather pere you're you mans. Deen't best Back as jlusnging misp.\n",
      "\n",
      "VAD.:\n",
      "If darni, hure whing to my des, carde have gets seed no, ghould be I dablo\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 003900:\tCurrent batch loss: 48.5094\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DANDIE:\n",
      "Nobes. goundingtfingd seldion Vight lit giver the lirg Shas of thell goilvertn be this on geick your up viscing. I'm are could whe fruf sembboe. Lidni.. think?  Cenout tiel teles.\n",
      "\n",
      "VODE:\n",
      "Whot'll biaty ded...\n",
      "\n",
      "MANTY:\n",
      "He'm, erast.\n",
      "\n",
      "LUKE:\n",
      "Howition jubl was  senin lo m\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 000000:\tCurrent batch loss: 70.0888\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SPFRERI hexp this that could ould you su'd man was sasith. I wat braw... cem.\n",
      "\n",
      "VIACH:\n",
      "Tagask whet of soll, to hoy good?\n",
      "\n",
      "DUKE:\n",
      "I do hoos. that's thy kect beau. I sure go.\n",
      "\n",
      "GOWER:\n",
      "That't this sen in me wat that a the for linghtes. I for quea.\n",
      "\n",
      "JEFINK:\n",
      "Here hit, I wothinn pl\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 000975:\tCurrent batch loss: 48.7706\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RAMALLIGA:\n",
      "Why't walf krount that canetay dow I lidg's in he's your, ita in's if I till.\n",
      "Mand!\n",
      "\n",
      "TABIT:\n",
      "I'll tho just maviybackny?\n",
      "\n",
      "DONN:\n",
      "Yes.\n",
      "\n",
      "DR. ACOOOLEA:\n",
      "Toming hon't the alliuht coule than noe tubifna widd town a leat backs?' hat you he sway. Oke lignesting want that. \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 001950:\tCurrent batch loss: 55.8251\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KARMAS:\n",
      "I tord to brawrod, harssture.  Sowe won reameare, soing.\n",
      "\n",
      "BLENS:\n",
      "I'm is cas of the manted.  We's in premberf. Back to ale loo.... Forty ovet themant I don'. The wankser, lockyor omirked tering gare  melon' listerat.\n",
      "\n",
      "KORBERI MARH THRAHTIE:\n",
      "Yeah,'s Wongane woy the h\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 002925:\tCurrent batch loss: 56.9853\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CHRMIN:\n",
      "What gecdied.\n",
      "\n",
      "LUKE:\n",
      "We can ose's couldadsing for it there we fryos hir terdice?\n",
      "\n",
      "DONND:\n",
      "I hot did I wat dowe the enemss at un you, Cooch. Robs telle in he ewer gon whe lifo. Aver, Lid the than pereon.\n",
      "\n",
      "LUSE:\n",
      "Whe're wsow.\n",
      "\n",
      "DICQMINK0O:\n",
      "Halo nese. I fuce widl goicgur\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 003900:\tCurrent batch loss: 47.9917\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DWEN:\n",
      "The ple pine asn.\n",
      "\n",
      "LUKE:\n",
      "My net they wained. CHETHEWIU That hot for at'll roighs to!\n",
      "\n",
      "LUKE:\n",
      "What of  Weverld. Aver you,  I momag Rien a luck so metam andie the know what you gainkingroine to boon.\n",
      "\n",
      "TRUNDO:\n",
      "You'w id bule limsser your. What?\n",
      "\n",
      "LUKE:\n",
      "Wund. Trif fores, Fr\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 000000:\tCurrent batch loss: 68.5492\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KIS Y:\n",
      "N.\n",
      "\n",
      "DUKE:\n",
      "I held thinks eod.  We'cl shave this me squapade's ast ay where \n",
      "\n",
      "ELACKER:\n",
      "Sorgh out chan the lin's shoeen is othen chel shith twingit, to my didr thather they don't likner you prave wit Meforam?\n",
      "\n",
      "ERGIOSTE:\n",
      "Wene wise telly spma am sfow ove wast thime of ne\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 000975:\tCurrent batch loss: 48.1016\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DDDEY:\n",
      "What you recm.\n",
      "\n",
      "DEY:\n",
      "Some, Savene?\n",
      "\n",
      "TEDEDICT UNPEFF:\n",
      "Cell a puck..\n",
      "\n",
      "FRICK:\n",
      "I case, grant.\n",
      "\n",
      "DATE:\n",
      "I have gaknun yous your you a inle we're moocry diss wore it cas is, barke. I want?  the havings it it uping ar, aboughs for he's onine and it ony assed hit mo donsture \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 001950:\tCurrent batch loss: 55.3356\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "JEFFY:\n",
      "Yeshes ford fagcere.\n",
      "\n",
      "INTO:\n",
      "How conch.\n",
      "\n",
      "MUPICK:\n",
      "Novers?\n",
      "\n",
      "BERLY:\n",
      "Way.  Roured you sfay.\n",
      "\n",
      "ACOR HABHT:\n",
      "I won're one ale han eve be.  you's though.  I've Foren What.\n",
      "\n",
      "MARELA:\n",
      "A. She sit would! Furuect you sorred arotho lust give saived to you Fayplole?\n",
      "\n",
      "JOEN:\n",
      "IT I know \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 002925:\tCurrent batch loss: 56.7971\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RUDRED:\n",
      "This fas trotion theree From rians a shif do you bly the aroce, siem? Sheur Jethietyon'I bice this deessen her I car have you high 'stare... feel.\n",
      "\n",
      "STERT:\n",
      "Who with.\n",
      "\n",
      "MONEY:\n",
      "Sherred with wourd  wengize I butred trroing to the havry and wonitho?\n",
      "\n",
      "LANDO:\n",
      "Onysbond.\n",
      "\n",
      "YO\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 003900:\tCurrent batch loss: 47.5640\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KON:\n",
      "You hangeniging andin.\n",
      "\n",
      "DONNIE:\n",
      "Leent can the wiren cesing..... furan what of me to cad my gnas your batr t yot resheracry. They. Cis dow foop cake ind the Myery. Gust cas.\n",
      "\n",
      "LUKE:\n",
      "You fited on your.\n",
      "\n",
      "TORESALLOAPER:\n",
      "Whe besh sthearn'... so the rand.\n",
      "\n",
      "RODACHRIH P:\n",
      "Slis.\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 000000:\tCurrent batch loss: 67.9119\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "VIPROF:\n",
      "That expro.\n",
      "\n",
      "SAWHITE:\n",
      "I chelo nother yourdony fuckion they whe was, ny that this that this we'll on do you cournban your rear. Noh whe stero it of the for the nabpee nakater me betullo agave a thing ap this inth some oy.\n",
      "\n",
      "MR. IESTUDK:\n",
      "What he con't you vicin go so \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 000975:\tCurrent batch loss: 47.4223\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MAULT:\n",
      "Do wantaredel.\n",
      "\n",
      "INGERY:\n",
      "I moan gill as thing cougse.  Lous Futticaten?\n",
      "\n",
      "EBFFFREMICK:\n",
      "Oh?\n",
      "\n",
      "DECKOROT MOT:\n",
      "I're that's se?\n",
      "\n",
      "DESK:\n",
      "Theraulicg thear to you.\n",
      "\n",
      "IPFREY:\n",
      "I werel ale... I gon't cat silif Visiting. I who and in a tol 188und, one they goocl.\n",
      "\n",
      "JERY:\n",
      "But bam alli\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 001950:\tCurrent batch loss: 54.8229\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "JOID:\n",
      "You wesen, I prontid. Goring, Mifuss?\n",
      "\n",
      "PRELD:\n",
      "It's  I going as dlet whee cald.\n",
      "\n",
      "PIEK:\n",
      "It pllesear stee tom sit net this not's beciose, is mane.\n",
      "\n",
      "REEPCONTY:\n",
      "Peeld?..  Harking.\n",
      "\n",
      "EDDY:\n",
      "I'm crarkin.\n",
      "\n",
      "SONNID:\n",
      "Don't hay day really?\n",
      "\n",
      "MERMEN:\n",
      "I're rightick?.  Sidin, Lighth a\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 002925:\tCurrent batch loss: 56.6362\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "IPAR:\n",
      "No.in't get lood!\n",
      "\n",
      "ECTAF:\n",
      "Yes. Dowe.\n",
      "\n",
      "REPINDUY:\n",
      "Pa I gutine Icapapling the likilg of there, could trang.\n",
      "\n",
      "IVE:\n",
      "Fon if us.\n",
      "\n",
      "HAN:\n",
      "No tunk att.\n",
      "\n",
      "ENNAE:\n",
      "Light invons to that welnowrith. Id not over sa you woully one thereche Chet's expit.\n",
      "\n",
      "LENITE:\n",
      "Oh is cilling. CI frong\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 003900:\tCurrent batch loss: 47.2048\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "OLDO:\n",
      "\n",
      "SANVICHE:\n",
      "The deag to list the mulion,  You's ...\n",
      "\n",
      "HAN:\n",
      "How it?\n",
      "\n",
      "ORETH:\n",
      "Don'w on peming Monthr, but herdaino at nikin' was ny cand...y right, appene you be?\n",
      "\n",
      "DONNIE:\n",
      "Sout is?\n",
      "\n",
      "LEISE:\n",
      "What Light I have fuen deftapch no highh do wore don't kis mine, head.  Mome.  That\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 000000:\tCurrent batch loss: 67.6116\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DDETY:\n",
      "We.\n",
      "\n",
      "ELARELLI\n",
      "ARG we, I liny refure got aake I know onos.\n",
      "\n",
      "\n",
      "GVERMIN:\n",
      "I wathe byer to tion.\n",
      "\n",
      "CHARLIEL:\n",
      "I know we trow onest...I ant riffuresthas of up goble, Momatys went Jeepsins.  Hot heas lesita in one.\n",
      "\n",
      "VID:\n",
      "AHhed a toutronl the fuld donsd akd hald.\n",
      "\n",
      "BUDDY:\n",
      "But w\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 000975:\tCurrent batch loss: 47.1380\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KANN:\n",
      "Nou'te lefe gave bet the havey Let and and you hanpimid ot bong happamse.  It, ensoullen.  Arone.\n",
      "\n",
      "HTOF:\n",
      "Merigun.\n",
      "\n",
      "FREDDY:\n",
      "Have a plough sorges. Souck?\n",
      "\n",
      "DESSID:\n",
      "Not you and do soll I Trutsoning.  I'm calf The ceem, bolens best.\n",
      "\n",
      "KRAND:\n",
      "Thound sherouscly ear.  That's \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 001950:\tCurrent batch loss: 54.3306\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SARBEA:\n",
      "Were.\n",
      "\n",
      "DESMON:\n",
      "He sure you did son?\n",
      "\n",
      "HARAH:\n",
      "You'll't . Ok, Thase!\n",
      "\n",
      "STREDDY:\n",
      "The copely.\n",
      "\n",
      "JAHN':\n",
      "Madinistol you sorel... the reeon this thask theme mean?\n",
      "\n",
      "JEFIY:\n",
      "Goon?\n",
      "\n",
      "YOOBEY:\n",
      "Who dit'll. Balking the likny kand... the wore rash now.\n",
      "\n",
      "JISE:\n",
      "Whinr ous that.  We'lllea\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 002925:\tCurrent batch loss: 56.4551\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KARBEY:\n",
      "We'm is a torde any whelppreo. You han so wit amp and be fio you on I wey ..ione. What's migcon't tho'st.\n",
      "\n",
      "INGAR:\n",
      "On't this atl'th, lyod abough 'versaies froght... abour your in of themang you for Hive.\n",
      "\n",
      "CINDY:\n",
      "Wigh the cang a wered Lithhe sane, I what oite noidace\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "> Saving to: ./rnn_model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 003900:\tCurrent batch loss: 46.9125\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "INDY:\n",
      "Gone can't like ubod.\n",
      "\n",
      "MAVERINDO:\n",
      "Whir come.\n",
      "\n",
      "DONNAE:\n",
      "Luth pought couss it supserfs.\n",
      "\n",
      "HANDY:\n",
      "Bunt, got ouge, I'm no neking. I'm out about leen.\n",
      "\n",
      "LEIA:\n",
      "No!  That beewn. I lo, haw. HER MHANNIVEwIY FIUN:\n",
      "It proin' shee that anyid I cave?\n",
      "\n",
      "DIGHT:\n",
      "There do, you're say Lea\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Current loss is not better than previous one\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000010:\tbatch: 000000:\tCurrent batch loss: 65.9173\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "rnn = run_language_model(dataset, max_epochs=10, sequence_length=dataset.sequence_length, print_every=975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, \"rb\") as f:\n",
    "    rnn = pickle.load(f)\n",
    "    seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "    n_sample = 300\n",
    "    sampled = sample(rnn, seed, n_sample, dataset)\n",
    "    print(''.join(sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
