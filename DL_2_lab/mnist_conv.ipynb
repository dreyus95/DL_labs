{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import skimage as ski\n",
    "import skimage.io\n",
    "\n",
    "from tensorflow.contrib.layers import xavier_initializer_conv2d as xavier_conv2d\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> CONFIG FILE </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"max_epochs\" : 20,\n",
    "    \"batch_size\" : 50,\n",
    "    \"lr_policy\" : 1e-4,\n",
    "    \"num_examples\" : mnist.train.images.shape[0],\n",
    "    \"weight_decay\" : 1e-3,\n",
    "    \"SAVE_DIR\" : \"output_mnist_conv/\"\n",
    "}\n",
    "\n",
    "OUTPUT_SHAPE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Utility methods </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(session, name, imgs, labels):\n",
    "    print(\"\\nRunning evaluation: \", name)\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = imgs.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_x = imgs[i*batch_size:(i+1)*batch_size, :]\n",
    "        batch_y = labels[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        learning_rate = float((num_batches - i)/(num_batches))\n",
    "        if (learning_rate <= 0.0):\n",
    "            learning_rate = 0.01\n",
    "            \n",
    "        data_dict = {x: batch_x, y: batch_y, weight_decay: config[\"weight_decay\"],\n",
    "                     is_training: False, lr: learning_rate}\n",
    "        loss_val, predicted = session.run([loss, logits], feed_dict=data_dict)\n",
    "\n",
    "        yp = np.argmax(predicted, 1)\n",
    "        yt = np.argmax(batch_y, 1)\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "        loss_avg += loss_val\n",
    "        \n",
    "    acc = (cnt_correct / num_examples * 100)\n",
    "    loss_avg /= num_batches\n",
    "    print(name + \" accuracy = %.4f\" % acc)\n",
    "    print(name + \" avg loss = %.4f\\n\" % loss_avg)\n",
    "\n",
    "    \n",
    "def draw_conv_filters(epoch, step, weights, save_dir):\n",
    "  # kxkxCxn_filters\n",
    "  k, k, C, num_filters = weights.shape\n",
    "\n",
    "  w = weights.copy().swapaxes(0, 3).swapaxes(1,2)\n",
    "  w = w.reshape(num_filters, C, k, k)\n",
    "  w -= w.min()\n",
    "  w /= w.max()\n",
    "\n",
    "  border = 1\n",
    "  cols = 8\n",
    "  rows = math.ceil(num_filters / cols)\n",
    "  width = cols * k + (cols-1) * border\n",
    "  height = rows * k + (rows-1) * border\n",
    "\n",
    "  for i in range(1):\n",
    "    img = np.zeros([height, width])\n",
    "    for j in range(num_filters):\n",
    "      r = int(j / cols) * (k + border)\n",
    "      c = int(j % cols) * (k + border)\n",
    "      img[r:r+k,c:c+k] = w[j,i]\n",
    "    filename = 'epoch_%02d_step_%06d_input_%03d.png' % (epoch, step, i)\n",
    "    ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "    \n",
    "    \n",
    "def batch_norm(inputs, is_training, dims, decay=0.999, epsilon=1e-3):\n",
    "\n",
    "        scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "        print(\"Scale: \", scale.get_shape())\n",
    "        beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "        print(\"Beta: \",beta.get_shape())\n",
    "        pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "        print(\"Pop mean: \",pop_mean.get_shape())\n",
    "        pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "        print(\"Pop var: \",pop_var.get_shape())\n",
    "\n",
    "        def if_true():\n",
    "            batch_mean, batch_var = tf.nn.moments(inputs, dims)\n",
    "            print(\"Batch mean/var\", batch_mean.get_shape(), batch_var.get_shape())\n",
    "            train_mean = tf.assign(pop_mean,\n",
    "                                   pop_mean * decay + batch_mean * (1 - decay))\n",
    "            train_var = tf.assign(pop_var,\n",
    "                                  pop_var * decay + batch_var * (1 - decay))\n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                return tf.nn.batch_normalization(inputs,\n",
    "                                                 batch_mean, batch_var, beta, scale, epsilon)\n",
    "\n",
    "        def if_false():\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                                             pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "        result = tf.cond(is_training, if_true, if_false)\n",
    "        return result\n",
    "\n",
    "    \n",
    "def conv_2d(tensor, filters, biases, is_training=None, dims=None,strides=1, activation=tf.nn.relu, padding='SAME'):\n",
    "    h1 = tf.nn.conv2d(tensor, filters, strides=[1, strides, strides, 1], padding=padding)\n",
    "    h1 = tf.nn.bias_add(h1, biases)\n",
    "    return activation(batch_norm(h1, is_training, dims))\n",
    "\n",
    "\n",
    "def max_pool_2d(tensor, k_size=2, stride=2, padding='SAME'):\n",
    "    return tf.nn.max_pool(tensor, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding=padding)\n",
    "\n",
    "\n",
    "def dropout(tensor, use_dropout, rate=0.5):\n",
    "    return tf.layers.dropout(tensor, rate=rate, training=use_dropout)\n",
    "\n",
    "\n",
    "# FC layer\n",
    "def dense(tensor, filters, biases, is_training=None, dims=None, activation=None):\n",
    "    tensor = tf.reshape(tensor, [-1, filters.get_shape().as_list()[0]])\n",
    "    res = tf.matmul(tensor, filters) + biases\n",
    "    if activation:\n",
    "        return activation(batch_norm(res, is_training, dims))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Weights & Biases </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'conv1': tf.get_variable('w_conv1', [5, 5, 1, 16], initializer=xavier_conv2d()),\n",
    "    'conv2': tf.get_variable('w_conv2', [5, 5, 16, 32], initializer=xavier_conv2d()),\n",
    "\n",
    "    'fc3': tf.get_variable('w_fc3', [7 * 7 * 32, 512], initializer=xavier_conv2d()),\n",
    "    'fc4': tf.get_variable('w_fc4', [512, OUTPUT_SHAPE], initializer=xavier_conv2d())\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'conv1': tf.Variable(tf.zeros([16]), name='b_conv1'),\n",
    "    'conv2': tf.Variable(tf.zeros([32]), name='b_conv2'),\n",
    "    'fc3': tf.Variable(tf.zeros([512]), name='b_fc3'),\n",
    "    'fc4': tf.Variable(tf.zeros([OUTPUT_SHAPE]), name='b_fc4')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> LAYERS </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale:  (16,)\n",
      "Beta:  (16,)\n",
      "Pop mean:  (16,)\n",
      "Pop var:  (16,)\n",
      "Batch mean/var (16,) (16,)\n",
      "H1: (?, 28, 28, 16)\n",
      "H1 pooled: (?, 14, 14, 16)\n",
      "Scale:  (32,)\n",
      "Beta:  (32,)\n",
      "Pop mean:  (32,)\n",
      "Pop var:  (32,)\n",
      "Batch mean/var (32,) (32,)\n",
      "H2: (?, 14, 14, 32)\n",
      "H2 pooled: (?, 7, 7, 32)\n",
      "Scale:  (512,)\n",
      "Beta:  (512,)\n",
      "Pop mean:  (512,)\n",
      "Pop var:  (512,)\n",
      "Batch mean/var () ()\n",
      "FC1: (?, 512)\n",
      "Logits: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, shape=[None, OUTPUT_SHAPE])\n",
    "weight_decay = tf.placeholder(tf.float32)\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "#################### 1ST LAYER ####################\n",
    "input_x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# h1 is [batch_size, 28, 28, 16]\n",
    "h1 = conv_2d(input_x, weights[\"conv1\"], biases[\"conv1\"], is_training=is_training, dims=[0, 1, 2])\n",
    "print(\"H1:\", h1.get_shape())\n",
    "\n",
    "# max pool convolved layer [batch_size, 14, 14, 16]\n",
    "h1_pooled = max_pool_2d(h1, k_size=2)\n",
    "print(\"H1 pooled:\", h1_pooled.get_shape())\n",
    "\n",
    "#################### 2ND LAYER ####################\n",
    "# h2 is [batch_size, 14, 14, 32]\n",
    "h2 = conv_2d(h1_pooled, weights[\"conv2\"], biases[\"conv2\"], is_training=is_training, dims=[0, 1, 2])\n",
    "print(\"H2:\", h2.get_shape())\n",
    "\n",
    "# max pool convolved layer [batch_size, 7, 7, 32]\n",
    "h2_pooled = max_pool_2d(h2, k_size=2)\n",
    "print(\"H2 pooled:\", h2_pooled.get_shape())\n",
    "\n",
    "#################### FC LAYER ####################\n",
    "fc1 = dense(h2_pooled, weights['fc3'],  biases['fc3'], is_training=is_training, dims=[0, 1], activation=tf.nn.relu)\n",
    "print(\"FC1:\", fc1.get_shape())\n",
    "\n",
    "#################### FC LAYER ####################\n",
    "# logits is [batch_size, 10]\n",
    "logits = dense(fc1, weights['fc4'],  biases['fc4'], is_training=is_training, activation=None)\n",
    "print(\"Logits:\", logits.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Loss & Train step </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizers = 0\n",
    "for w in weights.values():\n",
    "    regularizers += tf.nn.l2_loss(w)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "loss = loss + weight_decay*regularizers\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MAIN CODE </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 100/1100, batch loss = 0.8433, train acc = 63.1400, lr=0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dreyus95/Python/tf_environments/tf3/lib/python3.5/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 200/1100, batch loss = 0.6799, train acc = 77.8600, lr=0.728\n",
      "epoch 1, step 300/1100, batch loss = 0.5857, train acc = 83.8200, lr=0.637\n",
      "epoch 1, step 400/1100, batch loss = 0.5253, train acc = 86.9200, lr=0.546\n",
      "epoch 1, step 500/1100, batch loss = 0.4330, train acc = 88.9280, lr=0.455\n",
      "epoch 1, step 600/1100, batch loss = 0.4020, train acc = 90.2833, lr=0.365\n",
      "epoch 1, step 700/1100, batch loss = 0.3633, train acc = 91.3543, lr=0.274\n",
      "epoch 1, step 800/1100, batch loss = 0.3831, train acc = 92.1800, lr=0.183\n",
      "epoch 1, step 900/1100, batch loss = 0.3313, train acc = 92.8578, lr=0.092\n",
      "epoch 1, step 1000/1100, batch loss = 0.3890, train acc = 93.4080, lr=0.001\n",
      "epoch 1, step 1100/1100, batch loss = 0.3572, train acc = 93.8364, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 97.0000\n",
      "Validation avg loss = 0.5987\n",
      "\n",
      "epoch 2, step 100/1100, batch loss = 0.4701, train acc = 91.4800, lr=0.819\n",
      "epoch 2, step 200/1100, batch loss = 0.3334, train acc = 94.0100, lr=0.728\n",
      "epoch 2, step 300/1100, batch loss = 0.3047, train acc = 95.0600, lr=0.637\n",
      "epoch 2, step 400/1100, batch loss = 0.2891, train acc = 95.6050, lr=0.546\n",
      "epoch 2, step 500/1100, batch loss = 0.2210, train acc = 96.0000, lr=0.455\n",
      "epoch 2, step 600/1100, batch loss = 0.3624, train acc = 96.3800, lr=0.365\n",
      "epoch 2, step 700/1100, batch loss = 0.3181, train acc = 96.6571, lr=0.274\n",
      "epoch 2, step 800/1100, batch loss = 0.2213, train acc = 96.9100, lr=0.183\n",
      "epoch 2, step 900/1100, batch loss = 0.3092, train acc = 97.1111, lr=0.092\n",
      "epoch 2, step 1000/1100, batch loss = 0.1937, train acc = 97.2540, lr=0.001\n",
      "epoch 2, step 1100/1100, batch loss = 0.2264, train acc = 97.4091, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 97.5200\n",
      "Validation avg loss = 0.4043\n",
      "\n",
      "epoch 3, step 100/1100, batch loss = 0.2539, train acc = 90.9200, lr=0.819\n",
      "epoch 3, step 200/1100, batch loss = 0.2337, train acc = 94.1100, lr=0.728\n",
      "epoch 3, step 300/1100, batch loss = 0.2020, train acc = 95.3467, lr=0.637\n",
      "epoch 3, step 400/1100, batch loss = 0.2048, train acc = 95.9900, lr=0.546\n",
      "epoch 3, step 500/1100, batch loss = 0.2336, train acc = 96.3840, lr=0.455\n",
      "epoch 3, step 600/1100, batch loss = 0.1842, train acc = 96.6967, lr=0.365\n",
      "epoch 3, step 700/1100, batch loss = 0.1701, train acc = 96.9743, lr=0.274\n",
      "epoch 3, step 800/1100, batch loss = 0.1603, train acc = 97.1925, lr=0.183\n",
      "epoch 3, step 900/1100, batch loss = 0.1692, train acc = 97.3756, lr=0.092\n",
      "epoch 3, step 1000/1100, batch loss = 0.1585, train acc = 97.5180, lr=0.001\n",
      "epoch 3, step 1100/1100, batch loss = 0.1329, train acc = 97.6564, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 97.9800\n",
      "Validation avg loss = 0.2781\n",
      "\n",
      "epoch 4, step 100/1100, batch loss = 0.2823, train acc = 85.0000, lr=0.819\n",
      "epoch 4, step 200/1100, batch loss = 0.2781, train acc = 90.7800, lr=0.728\n",
      "epoch 4, step 300/1100, batch loss = 0.2873, train acc = 93.0000, lr=0.637\n",
      "epoch 4, step 400/1100, batch loss = 0.2460, train acc = 94.2850, lr=0.546\n",
      "epoch 4, step 500/1100, batch loss = 0.1735, train acc = 95.0720, lr=0.455\n",
      "epoch 4, step 600/1100, batch loss = 0.1606, train acc = 95.6233, lr=0.365\n",
      "epoch 4, step 700/1100, batch loss = 0.1497, train acc = 96.0400, lr=0.274\n",
      "epoch 4, step 800/1100, batch loss = 0.2028, train acc = 96.3800, lr=0.183\n",
      "epoch 4, step 900/1100, batch loss = 0.1514, train acc = 96.6622, lr=0.092\n",
      "epoch 4, step 1000/1100, batch loss = 0.2479, train acc = 96.8720, lr=0.001\n",
      "epoch 4, step 1100/1100, batch loss = 0.2393, train acc = 97.0491, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.0400\n",
      "Validation avg loss = 0.2298\n",
      "\n",
      "epoch 5, step 100/1100, batch loss = 0.2104, train acc = 96.9000, lr=0.819\n",
      "epoch 5, step 200/1100, batch loss = 0.1740, train acc = 97.3800, lr=0.728\n",
      "epoch 5, step 300/1100, batch loss = 0.1291, train acc = 97.5867, lr=0.637\n",
      "epoch 5, step 400/1100, batch loss = 0.1552, train acc = 97.7200, lr=0.546\n",
      "epoch 5, step 500/1100, batch loss = 0.1810, train acc = 97.8920, lr=0.455\n",
      "epoch 5, step 600/1100, batch loss = 0.1094, train acc = 97.9533, lr=0.365\n",
      "epoch 5, step 700/1100, batch loss = 0.1149, train acc = 98.1086, lr=0.274\n",
      "epoch 5, step 800/1100, batch loss = 0.0970, train acc = 98.2125, lr=0.183\n",
      "epoch 5, step 900/1100, batch loss = 0.1178, train acc = 98.3022, lr=0.092\n",
      "epoch 5, step 1000/1100, batch loss = 0.0977, train acc = 98.4020, lr=0.001\n",
      "epoch 5, step 1100/1100, batch loss = 0.0920, train acc = 98.4691, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.3000\n",
      "Validation avg loss = 0.1720\n",
      "\n",
      "epoch 6, step 100/1100, batch loss = 0.1468, train acc = 95.9600, lr=0.819\n",
      "epoch 6, step 200/1100, batch loss = 0.1639, train acc = 96.7400, lr=0.728\n",
      "epoch 6, step 300/1100, batch loss = 0.1337, train acc = 97.1667, lr=0.637\n",
      "epoch 6, step 400/1100, batch loss = 0.1036, train acc = 97.3800, lr=0.546\n",
      "epoch 6, step 500/1100, batch loss = 0.1828, train acc = 97.5480, lr=0.455\n",
      "epoch 6, step 600/1100, batch loss = 0.1941, train acc = 97.7533, lr=0.365\n",
      "epoch 6, step 700/1100, batch loss = 0.1137, train acc = 97.9257, lr=0.274\n",
      "epoch 6, step 800/1100, batch loss = 0.0890, train acc = 98.0375, lr=0.183\n",
      "epoch 6, step 900/1100, batch loss = 0.1043, train acc = 98.1533, lr=0.092\n",
      "epoch 6, step 1000/1100, batch loss = 0.1785, train acc = 98.2540, lr=0.001\n",
      "epoch 6, step 1100/1100, batch loss = 0.1520, train acc = 98.3436, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.5000\n",
      "Validation avg loss = 0.1352\n",
      "\n",
      "epoch 7, step 100/1100, batch loss = 0.1011, train acc = 96.8200, lr=0.819\n",
      "epoch 7, step 200/1100, batch loss = 0.1652, train acc = 97.2800, lr=0.728\n",
      "epoch 7, step 300/1100, batch loss = 0.1142, train acc = 97.5333, lr=0.637\n",
      "epoch 7, step 400/1100, batch loss = 0.0845, train acc = 97.7150, lr=0.546\n",
      "epoch 7, step 500/1100, batch loss = 0.0917, train acc = 97.8680, lr=0.455\n",
      "epoch 7, step 600/1100, batch loss = 0.0808, train acc = 98.0400, lr=0.365\n",
      "epoch 7, step 700/1100, batch loss = 0.0739, train acc = 98.1857, lr=0.274\n",
      "epoch 7, step 800/1100, batch loss = 0.0733, train acc = 98.3050, lr=0.183\n",
      "epoch 7, step 900/1100, batch loss = 0.0834, train acc = 98.3689, lr=0.092\n",
      "epoch 7, step 1000/1100, batch loss = 0.0677, train acc = 98.4540, lr=0.001\n",
      "epoch 7, step 1100/1100, batch loss = 0.0684, train acc = 98.5273, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.6800\n",
      "Validation avg loss = 0.1189\n",
      "\n",
      "epoch 8, step 100/1100, batch loss = 0.0819, train acc = 97.4600, lr=0.819\n",
      "epoch 8, step 200/1100, batch loss = 0.1649, train acc = 97.5200, lr=0.728\n",
      "epoch 8, step 300/1100, batch loss = 0.1205, train acc = 97.7667, lr=0.637\n",
      "epoch 8, step 400/1100, batch loss = 0.1143, train acc = 97.8750, lr=0.546\n",
      "epoch 8, step 500/1100, batch loss = 0.1032, train acc = 98.0840, lr=0.455\n",
      "epoch 8, step 600/1100, batch loss = 0.0718, train acc = 98.2167, lr=0.365\n",
      "epoch 8, step 700/1100, batch loss = 0.0728, train acc = 98.3314, lr=0.274\n",
      "epoch 8, step 800/1100, batch loss = 0.0659, train acc = 98.4300, lr=0.183\n",
      "epoch 8, step 900/1100, batch loss = 0.1801, train acc = 98.5089, lr=0.092\n",
      "epoch 8, step 1000/1100, batch loss = 0.0948, train acc = 98.5860, lr=0.001\n",
      "epoch 8, step 1100/1100, batch loss = 0.0732, train acc = 98.6491, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.9400\n",
      "Validation avg loss = 0.1039\n",
      "\n",
      "epoch 9, step 100/1100, batch loss = 0.1813, train acc = 96.5200, lr=0.819\n",
      "epoch 9, step 200/1100, batch loss = 0.0875, train acc = 97.3900, lr=0.728\n",
      "epoch 9, step 300/1100, batch loss = 0.0838, train acc = 97.6200, lr=0.637\n",
      "epoch 9, step 400/1100, batch loss = 0.0838, train acc = 97.8550, lr=0.546\n",
      "epoch 9, step 500/1100, batch loss = 0.0836, train acc = 98.0440, lr=0.455\n",
      "epoch 9, step 600/1100, batch loss = 0.0713, train acc = 98.1433, lr=0.365\n",
      "epoch 9, step 700/1100, batch loss = 0.1048, train acc = 98.2514, lr=0.274\n",
      "epoch 9, step 800/1100, batch loss = 0.1116, train acc = 98.4025, lr=0.183\n",
      "epoch 9, step 900/1100, batch loss = 0.0725, train acc = 98.5311, lr=0.092\n",
      "epoch 9, step 1000/1100, batch loss = 0.0683, train acc = 98.6160, lr=0.001\n",
      "epoch 9, step 1100/1100, batch loss = 0.0708, train acc = 98.6582, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.0800\n",
      "Validation avg loss = 0.0973\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, step 100/1100, batch loss = 0.1749, train acc = 93.5400, lr=0.819\n",
      "epoch 10, step 200/1100, batch loss = 0.1450, train acc = 95.7000, lr=0.728\n",
      "epoch 10, step 300/1100, batch loss = 0.1123, train acc = 96.4933, lr=0.637\n",
      "epoch 10, step 400/1100, batch loss = 0.1418, train acc = 96.9750, lr=0.546\n",
      "epoch 10, step 500/1100, batch loss = 0.0953, train acc = 97.2840, lr=0.455\n",
      "epoch 10, step 600/1100, batch loss = 0.1197, train acc = 97.5533, lr=0.365\n",
      "epoch 10, step 700/1100, batch loss = 0.2062, train acc = 97.7371, lr=0.274\n",
      "epoch 10, step 800/1100, batch loss = 0.0789, train acc = 97.9525, lr=0.183\n",
      "epoch 10, step 900/1100, batch loss = 0.0805, train acc = 98.0844, lr=0.092\n",
      "epoch 10, step 1000/1100, batch loss = 0.1011, train acc = 98.2060, lr=0.001\n",
      "epoch 10, step 1100/1100, batch loss = 0.1529, train acc = 98.3091, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.9600\n",
      "Validation avg loss = 0.1170\n",
      "\n",
      "epoch 11, step 100/1100, batch loss = 22.7695, train acc = 32.9400, lr=0.819\n",
      "epoch 11, step 200/1100, batch loss = 19.1125, train acc = 37.7700, lr=0.728\n",
      "epoch 11, step 300/1100, batch loss = 16.5406, train acc = 46.1267, lr=0.637\n",
      "epoch 11, step 400/1100, batch loss = 14.7549, train acc = 52.5450, lr=0.546\n",
      "epoch 11, step 500/1100, batch loss = 13.0236, train acc = 57.5400, lr=0.455\n",
      "epoch 11, step 600/1100, batch loss = 12.0058, train acc = 61.5500, lr=0.365\n",
      "epoch 11, step 700/1100, batch loss = 11.0808, train acc = 64.7657, lr=0.274\n",
      "epoch 11, step 800/1100, batch loss = 10.6913, train acc = 67.2975, lr=0.183\n",
      "epoch 11, step 900/1100, batch loss = 10.5860, train acc = 69.3822, lr=0.092\n",
      "epoch 11, step 1000/1100, batch loss = 10.2414, train acc = 71.1020, lr=0.001\n",
      "epoch 11, step 1100/1100, batch loss = 10.5980, train acc = 72.4564, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 84.8600\n",
      "Validation avg loss = 10.5115\n",
      "\n",
      "epoch 12, step 100/1100, batch loss = 8.9700, train acc = 70.4800, lr=0.819\n",
      "epoch 12, step 200/1100, batch loss = 7.8875, train acc = 76.7800, lr=0.728\n",
      "epoch 12, step 300/1100, batch loss = 6.6799, train acc = 79.9333, lr=0.637\n",
      "epoch 12, step 400/1100, batch loss = 6.0384, train acc = 82.1050, lr=0.546\n",
      "epoch 12, step 500/1100, batch loss = 5.3524, train acc = 83.6480, lr=0.455\n",
      "epoch 12, step 600/1100, batch loss = 4.9274, train acc = 84.8000, lr=0.365\n",
      "epoch 12, step 700/1100, batch loss = 4.8001, train acc = 85.7257, lr=0.274\n",
      "epoch 12, step 800/1100, batch loss = 4.3679, train acc = 86.5825, lr=0.183\n",
      "epoch 12, step 900/1100, batch loss = 4.2841, train acc = 87.2867, lr=0.092\n",
      "epoch 12, step 1000/1100, batch loss = 4.2888, train acc = 87.9320, lr=0.001\n",
      "epoch 12, step 1100/1100, batch loss = 4.2996, train acc = 88.3891, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 42.8600\n",
      "Validation avg loss = 5.7149\n",
      "\n",
      "epoch 13, step 100/1100, batch loss = 4.3462, train acc = 58.1600, lr=0.819\n",
      "epoch 13, step 200/1100, batch loss = 3.5510, train acc = 70.8900, lr=0.728\n",
      "epoch 13, step 300/1100, batch loss = 3.1182, train acc = 77.2400, lr=0.637\n",
      "epoch 13, step 400/1100, batch loss = 2.6856, train acc = 80.7750, lr=0.546\n",
      "epoch 13, step 500/1100, batch loss = 2.3957, train acc = 83.3480, lr=0.455\n",
      "epoch 13, step 600/1100, batch loss = 2.1735, train acc = 85.1733, lr=0.365\n",
      "epoch 13, step 700/1100, batch loss = 2.0611, train acc = 86.4857, lr=0.274\n",
      "epoch 13, step 800/1100, batch loss = 2.1299, train acc = 87.4450, lr=0.183\n",
      "epoch 13, step 900/1100, batch loss = 1.9092, train acc = 88.2933, lr=0.092\n",
      "epoch 13, step 1000/1100, batch loss = 1.8296, train acc = 89.0360, lr=0.001\n",
      "epoch 13, step 1100/1100, batch loss = 1.8638, train acc = 89.6018, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 20.9800\n",
      "Validation avg loss = 6.2125\n",
      "\n",
      "epoch 14, step 100/1100, batch loss = 2.0770, train acc = 76.2600, lr=0.819\n",
      "epoch 14, step 200/1100, batch loss = 1.5972, train acc = 84.2800, lr=0.728\n",
      "epoch 14, step 300/1100, batch loss = 1.4864, train acc = 87.1600, lr=0.637\n",
      "epoch 14, step 400/1100, batch loss = 1.2221, train acc = 88.9000, lr=0.546\n",
      "epoch 14, step 500/1100, batch loss = 1.1000, train acc = 90.0960, lr=0.455\n",
      "epoch 14, step 600/1100, batch loss = 0.9486, train acc = 90.9767, lr=0.365\n",
      "epoch 14, step 700/1100, batch loss = 1.0103, train acc = 91.7314, lr=0.274\n",
      "epoch 14, step 800/1100, batch loss = 0.9900, train acc = 92.2700, lr=0.183\n",
      "epoch 14, step 900/1100, batch loss = 0.8625, train acc = 92.7333, lr=0.092\n",
      "epoch 14, step 1000/1100, batch loss = 1.0953, train acc = 93.0800, lr=0.001\n",
      "epoch 14, step 1100/1100, batch loss = 0.8569, train acc = 93.4073, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 18.3000\n",
      "Validation avg loss = 3.5404\n",
      "\n",
      "epoch 15, step 100/1100, batch loss = 1.0063, train acc = 73.9600, lr=0.819\n",
      "epoch 15, step 200/1100, batch loss = 0.8375, train acc = 83.1600, lr=0.728\n",
      "epoch 15, step 300/1100, batch loss = 0.6742, train acc = 86.6733, lr=0.637\n",
      "epoch 15, step 400/1100, batch loss = 0.6112, train acc = 88.6600, lr=0.546\n",
      "epoch 15, step 500/1100, batch loss = 0.5145, train acc = 90.2200, lr=0.455\n",
      "epoch 15, step 600/1100, batch loss = 0.7218, train acc = 91.2567, lr=0.365\n",
      "epoch 15, step 700/1100, batch loss = 0.4880, train acc = 92.0714, lr=0.274\n",
      "epoch 15, step 800/1100, batch loss = 0.4692, train acc = 92.7450, lr=0.183\n",
      "epoch 15, step 900/1100, batch loss = 0.6060, train acc = 93.2533, lr=0.092\n",
      "epoch 15, step 1000/1100, batch loss = 0.5001, train acc = 93.7060, lr=0.001\n",
      "epoch 15, step 1100/1100, batch loss = 0.5685, train acc = 94.0145, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 67.3800\n",
      "Validation avg loss = 1.3874\n",
      "\n",
      "epoch 16, step 100/1100, batch loss = 0.7034, train acc = 70.5800, lr=0.819\n",
      "epoch 16, step 200/1100, batch loss = 0.7798, train acc = 81.7200, lr=0.728\n",
      "epoch 16, step 300/1100, batch loss = 0.4234, train acc = 85.9133, lr=0.637\n",
      "epoch 16, step 400/1100, batch loss = 0.5052, train acc = 88.3050, lr=0.546\n",
      "epoch 16, step 500/1100, batch loss = 0.4411, train acc = 90.0080, lr=0.455\n",
      "epoch 16, step 600/1100, batch loss = 0.3405, train acc = 91.0500, lr=0.365\n",
      "epoch 16, step 700/1100, batch loss = 0.3779, train acc = 91.9200, lr=0.274\n",
      "epoch 16, step 800/1100, batch loss = 0.3428, train acc = 92.5700, lr=0.183\n",
      "epoch 16, step 900/1100, batch loss = 0.4183, train acc = 93.1067, lr=0.092\n",
      "epoch 16, step 1000/1100, batch loss = 0.3019, train acc = 93.6260, lr=0.001\n",
      "epoch 16, step 1100/1100, batch loss = 0.2941, train acc = 93.9873, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 87.9600\n",
      "Validation avg loss = 0.7720\n",
      "\n",
      "epoch 17, step 100/1100, batch loss = 0.6997, train acc = 70.2800, lr=0.819\n",
      "epoch 17, step 200/1100, batch loss = 0.5531, train acc = 82.0700, lr=0.728\n",
      "epoch 17, step 300/1100, batch loss = 0.3264, train acc = 86.5600, lr=0.637\n",
      "epoch 17, step 400/1100, batch loss = 0.3196, train acc = 89.0000, lr=0.546\n",
      "epoch 17, step 500/1100, batch loss = 0.2541, train acc = 90.5480, lr=0.455\n",
      "epoch 17, step 600/1100, batch loss = 0.2801, train acc = 91.6533, lr=0.365\n",
      "epoch 17, step 700/1100, batch loss = 0.3437, train acc = 92.4029, lr=0.274\n",
      "epoch 17, step 800/1100, batch loss = 0.2482, train acc = 93.0225, lr=0.183\n",
      "epoch 17, step 900/1100, batch loss = 0.2112, train acc = 93.5689, lr=0.092\n",
      "epoch 17, step 1000/1100, batch loss = 0.3417, train acc = 94.0200, lr=0.001\n",
      "epoch 17, step 1100/1100, batch loss = 0.3273, train acc = 94.3909, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 74.9000\n",
      "Validation avg loss = 0.9023\n",
      "\n",
      "epoch 18, step 100/1100, batch loss = 0.3626, train acc = 80.1600, lr=0.819\n",
      "epoch 18, step 200/1100, batch loss = 0.3397, train acc = 87.7600, lr=0.728\n",
      "epoch 18, step 300/1100, batch loss = 0.2886, train acc = 90.6867, lr=0.637\n",
      "epoch 18, step 400/1100, batch loss = 0.2507, train acc = 92.1950, lr=0.546\n",
      "epoch 18, step 500/1100, batch loss = 0.1904, train acc = 93.2000, lr=0.455\n",
      "epoch 18, step 600/1100, batch loss = 0.3397, train acc = 93.8800, lr=0.365\n",
      "epoch 18, step 700/1100, batch loss = 0.1960, train acc = 94.3686, lr=0.274\n",
      "epoch 18, step 800/1100, batch loss = 0.1645, train acc = 94.8050, lr=0.183\n",
      "epoch 18, step 900/1100, batch loss = 0.2240, train acc = 95.1644, lr=0.092\n",
      "epoch 18, step 1000/1100, batch loss = 0.2058, train acc = 95.4480, lr=0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, step 1100/1100, batch loss = 0.2750, train acc = 95.7018, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 93.2400\n",
      "Validation avg loss = 0.4655\n",
      "\n",
      "epoch 19, step 100/1100, batch loss = 0.5935, train acc = 82.9200, lr=0.819\n",
      "epoch 19, step 200/1100, batch loss = 0.2318, train acc = 89.1600, lr=0.728\n",
      "epoch 19, step 300/1100, batch loss = 0.3390, train acc = 91.5533, lr=0.637\n",
      "epoch 19, step 400/1100, batch loss = 0.2483, train acc = 92.9550, lr=0.546\n",
      "epoch 19, step 500/1100, batch loss = 0.2181, train acc = 93.6600, lr=0.455\n",
      "epoch 19, step 600/1100, batch loss = 0.1599, train acc = 94.2600, lr=0.365\n",
      "epoch 19, step 700/1100, batch loss = 0.1689, train acc = 94.8086, lr=0.274\n",
      "epoch 19, step 800/1100, batch loss = 0.1550, train acc = 95.2075, lr=0.183\n",
      "epoch 19, step 900/1100, batch loss = 0.1771, train acc = 95.5689, lr=0.092\n",
      "epoch 19, step 1000/1100, batch loss = 0.1326, train acc = 95.8540, lr=0.001\n",
      "epoch 19, step 1100/1100, batch loss = 0.1386, train acc = 96.0509, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 96.1800\n",
      "Validation avg loss = 0.2971\n",
      "\n",
      "epoch 20, step 100/1100, batch loss = 0.7442, train acc = 64.5800, lr=0.819\n",
      "epoch 20, step 200/1100, batch loss = 0.5607, train acc = 78.9100, lr=0.728\n",
      "epoch 20, step 300/1100, batch loss = 0.5557, train acc = 84.2933, lr=0.637\n",
      "epoch 20, step 400/1100, batch loss = 0.3740, train acc = 87.2200, lr=0.546\n",
      "epoch 20, step 500/1100, batch loss = 0.4017, train acc = 89.0560, lr=0.455\n",
      "epoch 20, step 600/1100, batch loss = 0.3540, train acc = 90.4300, lr=0.365\n",
      "epoch 20, step 700/1100, batch loss = 0.3461, train acc = 91.4457, lr=0.274\n",
      "epoch 20, step 800/1100, batch loss = 0.2775, train acc = 92.1825, lr=0.183\n",
      "epoch 20, step 900/1100, batch loss = 0.2945, train acc = 92.8067, lr=0.092\n",
      "epoch 20, step 1000/1100, batch loss = 0.2831, train acc = 93.3080, lr=0.001\n",
      "epoch 20, step 1100/1100, batch loss = 0.3829, train acc = 93.7327, lr=0.001\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 97.8000\n",
      "Validation avg loss = 0.3204\n",
      "\n",
      "\n",
      "Running evaluation:  Test\n",
      "Test accuracy = 97.9400\n",
      "Test avg loss = 0.3135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    max_epochs = config[\"max_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr_policy = config[\"lr_policy\"]\n",
    "    num_examples = mnist.train.images.shape[0]\n",
    "    num_batches = num_examples // batch_size\n",
    "\n",
    "    train_x = mnist.train.images\n",
    "    train_x = train_x.reshape([-1, 28, 28, 1])\n",
    "    train_y = mnist.train.labels\n",
    "\n",
    "    valid_x = mnist.validation.images\n",
    "    valid_x = valid_x.reshape([-1, 28, 28, 1])\n",
    "    valid_y = mnist.validation.labels\n",
    "\n",
    "    test_x = mnist.test.images\n",
    "    test_x = test_x.reshape([-1, 28, 28, 1])\n",
    "    test_y = mnist.test.labels\n",
    "\n",
    "    train_mean = train_x.mean()\n",
    "    train_x -= train_mean\n",
    "    valid_x -= train_mean\n",
    "    test_x -= train_mean\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        cnt_correct = 0\n",
    "\n",
    "        permutation_idx = np.random.permutation(num_examples)\n",
    "        train_x = train_x[permutation_idx]\n",
    "        train_y = train_y[permutation_idx]\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            # store mini-batch to ndarray\n",
    "            batch_x = train_x[i * batch_size:(i + 1) * batch_size, :]\n",
    "            batch_y = train_y[i * batch_size:(i + 1) * batch_size, :]\n",
    "\n",
    "            learning_rate = float((num_batches - i - 100)/(num_batches))\n",
    "            if (learning_rate <= 0.05):\n",
    "                learning_rate = 0.001\n",
    "                            \n",
    "            data_dict = {x: batch_x, y: batch_y, lr: learning_rate,\n",
    "                         weight_decay: config[\"weight_decay\"], is_training: True}\n",
    "            loss_val, predicted, _ = sess.run([loss, logits, train_step], feed_dict=data_dict)\n",
    "            \n",
    "            yp = np.argmax(predicted, 1)\n",
    "            yt = np.argmax(batch_y, 1)\n",
    "            cnt_correct += (yp == yt).sum()\n",
    "            acc = (cnt_correct / ((i+1)*batch_size) * 100)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                w = sess.run(weights['conv1'])\n",
    "                print(\"epoch %d, step %d/%d, batch loss = %.4f, train acc = %.4f, lr=%.3f\" %\n",
    "                      (epoch, i+1, num_batches, loss_val, acc, learning_rate))\n",
    "                \n",
    "                draw_conv_filters(epoch, i * batch_size, w, config[\"SAVE_DIR\"])\n",
    "\n",
    "        evaluate(sess, \"Validation\", valid_x, valid_y)\n",
    "        \n",
    "    # TESTING\n",
    "    evaluate(sess, \"Test\", test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
